# GPT-2 Clone - CPU-Optimized Training

A clean, educational implementation of GPT-2 optimized for training on consumer hardware (CPU/Intel Iris Plus Graphics). This is a learning-focused rewrite that prioritizes clarity and accessibility over raw performance, making it perfect for understanding transformer architecture and training your first language model.

## Features

- ðŸŽ“ **Educational Focus**: Clear, well-commented code that's easy to understand
- ðŸ’» **CPU Compatible**: Optimized hyperparameters for training on regular laptops
- ðŸ”§ **Flexible Architecture**: Easy to modify and experiment with
- ðŸ“Š **Multiple Model Sizes**: From tiny (24M params) to GPT-2 scale (124M params)
- ðŸš€ **Optional GPU Support**: Automatically uses CUDA/MPS when available
- ðŸŽ¯ **Complete Implementation**: Includes training, generation, and pretrained weight loading

## Project Structure

```
.
â”œâ”€â”€ model.py           # ~300 lines - Complete GPT model definition
â”œâ”€â”€ train.py           # ~200 lines - Training loop (simplified)
â”œâ”€â”€ input              # Your training data (text file)
â””â”€â”€ log/              # Training logs and checkpoints
```

## Quick Start

### Installation

```bash
pip install torch numpy transformers tiktoken
```

**Dependencies:**
- `torch` - PyTorch for neural networks
- `numpy` - Numerical operations  
- `transformers` - To load pretrained GPT-2 weights
- `tiktoken` - Fast BPE tokenization from OpenAI

### Training Your First Model

#### 1. Prepare Your Data

Create a text file named `input` with your training data:

```bash
# Example: Download Shakespeare
curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o input
```

#### 2. Train on CPU (Consumer Laptop)

The default configuration is optimized for CPU training:

```python
python train.py
```

**Default Settings (CPU-Optimized):**
- Model: 6 layers, 6 heads, 384 embedding dim (~24M parameters)
- Batch size: 4
- Sequence length: 256 tokens
- Training steps: 50 (for quick testing)

**Expected Performance:**
- ~5-20 seconds per step on modern CPU
- Training completes in ~5-15 minutes

#### 3. Train on GPU (If Available)

The code automatically detects and uses CUDA/MPS:

```python
# Automatically uses GPU if available
python train.py

# Or force CPU
device = 'cpu'  # Change in script
```

### Generation / Sampling

Modify the generation code at the bottom of `train.py` (currently after `sys.exit(0)`):

```python
# Remove or comment out: import sys; sys.exit(0)

# The generation code will then run:
model.eval()
num_return_sequences = 5
max_length = 30

tokens = enc.encode("Hello, I'm a language model,")
tokens = torch.tensor(tokens, dtype=torch.long)
tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)
x = tokens.to(device)

x = model.generate(x, max_new_tokens=max_length, temperature=1.0, top_k=50)

for i in range(num_return_sequences):
    decoded = enc.decode(x[i].tolist())
    print(f"{i+1}> {decoded}")
```

## Configuration

### Model Sizes

Edit the `GPTConfig` in your script:

```python
# Tiny model (CPU-friendly, ~6M params)
model = GPT(GPTConfig(
    vocab_size=50304,
    n_layer=4,
    n_head=4,
    n_embd=256,
    block_size=256
))

# Small model (Default, ~24M params)
model = GPT(GPTConfig(
    vocab_size=50304,
    n_layer=6,
    n_head=6,
    n_embd=384,
    block_size=256
))

# GPT-2 Small (124M params - requires GPU)
model = GPT(GPTConfig(
    vocab_size=50304,
    n_layer=12,
    n_head=12,
    n_embd=768,
    block_size=1024
))
```

### Training Hyperparameters

```python
# Batch settings
total_batch_size = 8192    # Total tokens per update
B = 4                      # Micro batch size
T = 256                    # Sequence length
grad_accum_steps = 8       # Gradient accumulation steps

# Learning rate schedule
max_lr = 6e-4              # Peak learning rate
min_lr = max_lr * 0.1      # Minimum learning rate
warmup_steps = 10          # Warmup iterations
max_steps = 50             # Total training steps

# Optimization
weight_decay = 0.1         # L2 regularization
betas = (0.9, 0.95)       # Adam betas
```

## Loading Pretrained GPT-2 Weights

```python
# Load official GPT-2 weights from OpenAI
model = GPT.from_pretrained('gpt2')        # 124M params
model = GPT.from_pretrained('gpt2-medium') # 350M params
model = GPT.from_pretrained('gpt2-large')  # 774M params
model = GPT.from_pretrained('gpt2-xl')     # 1558M params

model.eval()
model.to(device)

# Generate
x = model.generate(tokens, max_new_tokens=100, temperature=0.8, top_k=50)
```

## Performance Benchmarks

### CPU (Intel Core i5-i7, Consumer Laptop)
| Model Size | Parameters | Step Time | Tokens/sec |
|------------|-----------|-----------|------------|
| Tiny (4L)  | ~6M       | 2-5s      | 200-500    |
| Small (6L) | ~24M      | 5-15s     | 70-200     |
| GPT-2 (12L)| ~124M     | 30-90s    | 10-35      |

### GPU (NVIDIA RTX 3060)
| Model Size | Parameters | Step Time | Tokens/sec |
|------------|-----------|-----------|------------|
| Small (6L) | ~24M      | 100-200ms | 5,000-10,000 |
| GPT-2 (12L)| ~124M     | 300-500ms | 2,000-3,000  |

*Note: CPU training is 50-100x slower than GPU training*

## Hardware Requirements

### Minimum (CPU Training)
- **CPU**: Any modern x64 processor
- **RAM**: 8GB (4GB model + data)
- **Storage**: 1GB for model + data
- **Time**: ~15 minutes for 50 steps on small model

### Recommended (CPU Training)
- **CPU**: Intel i5/i7 or AMD Ryzen 5/7
- **RAM**: 16GB
- **Storage**: 5GB
- **Time**: ~5-10 minutes for 50 steps

### GPU Training (Optional)
- **GPU**: NVIDIA GPU with 6GB+ VRAM or Apple Silicon (M1/M2)
- **CUDA**: PyTorch with CUDA 11.8+ or MPS support
- **Time**: ~30 seconds for 50 steps

## Known Limitations

### CPU-Specific Issues

1. **No bfloat16 Support**: CPU doesn't support mixed precision training
   - Solution: Runs in float32 (already implemented)

2. **No Tensor Cores**: `torch.set_float32_matmul_precision()` doesn't work
   - Solution: Automatically disabled on CPU

3. **Slow Training**: 50-100x slower than GPU
   - Solution: Use smaller models (4-6 layers) and reduced batch sizes

4. **Intel Iris Plus Graphics**: Not CUDA-compatible
   - Solution: Falls back to CPU automatically

### Workarounds Implemented

```python
# Automatic device detection
device = 'cpu'
if torch.cuda.is_available():
    device = 'cuda'
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = 'mps'

# Conditional mixed precision
if device_type == "cuda" and torch.cuda.is_bf16_supported():
    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
        logits, loss = model(x, y)
else:
    logits, loss = model(x, y)  # CPU uses float32

# Conditional synchronization
if device_type == "cuda":
    torch.cuda.synchronize()
```

## Troubleshooting

### Common Errors

**1. "RuntimeError: addmm_impl_cpu_ not implemented for BFloat16"**
```python
# Solution: Remove autocast or use CPU-compatible version (already fixed)
```

**2. "AttributeError: 'Tensor' object has no attribute 'synchronize'"**
```python
# Solution: Use torch.cuda.synchronize() not torch.cpu.synchronize()
```

**3. "CUDA out of memory"**
```python
# Reduce batch size or model size
B = 2  # Reduce from 4
T = 128  # Reduce from 256
n_layer = 4  # Reduce from 6
```

**4. Training is very slow on CPU**
```python
# This is expected. Optimize with:
- Reduce model size (4 layers, 256 dims)
- Reduce sequence length (T=128 or T=64)
- Reduce batch size (B=2)
- Use fewer training steps
```

## Architecture Details

This implementation includes:

- âœ… **Causal Self-Attention** with Flash Attention (when available)
- âœ… **Layer Normalization** with optional bias
- âœ… **Dropout Regularization** (attention + residual + embedding)
- âœ… **Weight Tying** (input embedding = output projection)
- âœ… **Gradient Accumulation** for larger effective batch sizes
- âœ… **Learning Rate Scheduling** (warmup + cosine decay)
- âœ… **Gradient Clipping** (max norm = 1.0)
- âœ… **Weight Decay** with parameter grouping
- âœ… **Top-k Sampling** for text generation

## Educational Resources

To learn more about the architecture:

1. **Attention is All You Need** (Original Transformer paper)
2. **GPT-2 Paper**: "Language Models are Unsupervised Multitask Learners"
3. **Andrej Karpathy's nanoGPT**: Original inspiration for this implementation
4. **The Illustrated Transformer** by Jay Alammar

## Credits

This implementation is inspired by:
- [nanoGPT](https://github.com/karpathy/nanoGPT) by Andrej Karpathy
- [minGPT](https://github.com/karpathy/minGPT) by Andrej Karpathy
- OpenAI's GPT-2 implementation

## License

MIT License - Free to use for educational purposes

## Contributing

This is an educational project. Feel free to:
- Report bugs or issues
- Suggest improvements
- Share your training results
- Create forks for your own experiments

## Contact

For questions or issues, please open a GitHub issue or reach out to the maintainers.

---

**Note**: This implementation prioritizes education and accessibility over performance. For production use, consider using the official [transformers](https://github.com/huggingface/transformers) library or [nanoGPT](https://github.com/karpathy/nanoGPT).
